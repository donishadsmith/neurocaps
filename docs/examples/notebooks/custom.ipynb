{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Using Lateralized Custom Parcellations\n",
    "While NeuroCAPs leverages Nilearn's fetch functions for the [Schaefer](https://nilearn.github.io/stable/modules/generated/nilearn.datasets.fetch_atlas_schaefer_2018.html)\n",
    "and [AAL](https://nilearn.github.io/stable/modules/generated/nilearn.datasets.fetch_atlas_aal.html), additional\n",
    "lateralized parcellations can be manually defined. For custom parcellation approaches, three subkeys are\n",
    "recognized: \"maps\", \"nodes\", and \"regions\". For additional details on these subkeys, refer to the\n",
    "[\"Custom Parcellations\" sub-section](https://neurocaps.readthedocs.io/en/stable/user_guide/parcellations.html#custom-parcellations).\n",
    "\n",
    "For this demonstration, the extended Human Connectome Project multimodal parcellation (HCPex) from\n",
    "[wayalan's Github](https://github.com/wayalan/HCPex/) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "!pip install neurocaps[windows,demo]\n",
    "\n",
    "import os, sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "  os.environ[\"DISPLAY\"] = \":0.0\"\n",
    "  !apt-get install -y xvfb\n",
    "  !Xvfb :0 -screen 0 1024x768x24 &> /dev/null &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching atlas NiFTI image and labels from Github\n",
    "import os, subprocess, sys\n",
    "\n",
    "demo_dir = \"neurocaps_demo\"\n",
    "os.makedirs(demo_dir, exist_ok=True)\n",
    "\n",
    "if sys.platform != \"win32\":\n",
    "    cmd = [\n",
    "        [\"wget\", \"-q\", \"-P\", demo_dir, \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex_LookUpTable.txt\"],\n",
    "        [\"wget\", \"-q\", \"-P\", demo_dir, \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex.nii.gz\"],\n",
    "    ]\n",
    "else:\n",
    "    cmd = [\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\",\n",
    "            f\"{demo_dir}\\\\HCPex_LookUpTable.txt\",\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex_LookUpTable.txt\",\n",
    "        ],\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\",\n",
    "            f\"{demo_dir}\\\\HCPex.nii.gz\",\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex.nii.gz\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "for command in cmd:\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below fetches a single subject from an [OpenNeuro dataset](https://openneuro.org/datasets/ds005381/versions/1.0.0)\n",
    "preprocessed with [fMRIPrep](https://fmriprep.org/en/stable/). Downloading data from OpenNeuro requires\n",
    "``pip install openneuro-py ipywidgets`` or ``pip install neurocaps[demo]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Dataset] doi: doi:10.18112/openneuro.ds005381.v1.0.0\n",
    "from openneuro import download\n",
    "\n",
    "# Include the run-1 and run-2 data of a single subject\n",
    "include = [\n",
    "    \"dataset_description.json\",\n",
    "    \"sub-0004/ses-2/func/*run-[12]*events*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]*confounds_timeseries*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]_space-MNI152NLin*preproc_bold*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]_space-MNI152NLin*brain_mask*\",\n",
    "]\n",
    "\n",
    "download(\n",
    "    dataset=\"ds005381\",\n",
    "    include=include,\n",
    "    target_dir=demo_dir,\n",
    "    verify_hash=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first level of the pipeline directory must also have a dataset_description.json file for querying purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "desc = {\n",
    "    \"Name\": \"fMRIPrep - fMRI PREProcessing workflow\",\n",
    "    \"BIDSVersion\": \"1.0.0\",\n",
    "    \"DatasetType\": \"derivative\",\n",
    "    \"GeneratedBy\": [{\"Name\": \"fMRIPrep\", \"Version\": \"20.2.0\", \"CodeURL\": \"https://github.com/nipreps/fmriprep\"}],\n",
    "}\n",
    "\n",
    "with open(\"neurocaps_demo/derivatives/fmriprep/dataset_description.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(desc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Timeseries\n",
    "\n",
    "For ``TimeseriesExtractor.get_bold``, only the \"maps\" subkey (the location of the parcellation) needs to be defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom parcel approach dictionary and extracting timeseries\n",
    "parcel_approach = {\"Custom\": {}}\n",
    "\n",
    "parcel_approach[\"Custom\"][\"maps\"] = os.path.join(demo_dir, \"HCPex.nii.gz\")\n",
    "\n",
    "from neurocaps.extraction import TimeseriesExtractor\n",
    "\n",
    "extractor = TimeseriesExtractor(\n",
    "    space=\"MNI152NLin6Asym\",\n",
    "    parcel_approach=parcel_approach,\n",
    "    standardize=\"zscore_sample\",\n",
    "    use_confounds=True,\n",
    "    confound_names=\"basic\",\n",
    "    detrend=True,\n",
    "    low_pass=0.15,\n",
    "    high_pass=None,\n",
    "    dummy_scans=5,\n",
    "    fd_threshold={\"threshold\": 0.5, \"outlier_percentage\": 0.30, \"use_sample_mask\": True, \"interpolate\": False},\n",
    ")\n",
    "\n",
    "# Using chaining to extract timeseries data and save dictionary as a pickle file\n",
    "extractor.get_bold(\n",
    "    bids_dir=\"neurocaps_demo\",\n",
    "    session=\"2\",\n",
    "    task=\"DET\",\n",
    "    condition=\"late\",\n",
    "    condition_tr_shift=2,\n",
    "    slice_time_ref=0.5,\n",
    "    n_cores=None,\n",
    "    flush=True,\n",
    "    tr=2,\n",
    "    verbose=True,\n",
    ").timeseries_to_pickle(output_dir=demo_dir, filename=\"openneuro_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization methods in the ``TimeseriesExtractor`` and ``CAP`` classes, the nodes and regions need to be defined.\n",
    "Refer to the documentation for each function to determine which subkeys are required, as some methods only need the\n",
    "\"maps\" subkey, while others require the \"nodes\" and \"regions\" subkeys.\n",
    "\n",
    "The following code defines the nodes and regions of the HCPex parcellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, pandas as pd\n",
    "\n",
    "# Setting the \"nodes\"; needed for `TimeseriesExtractor.visualize_bold`; Getting nodes that don't correspond to\n",
    "# background label\n",
    "parcel_approach[\"Custom\"][\"nodes\"] = pd.read_csv(\n",
    "    os.path.join(demo_dir, \"HCPex_LookUpTable.txt\"),\n",
    "    sep=None,\n",
    "    engine=\"python\",\n",
    ")[\"Label\"].values[1:]\n",
    "\n",
    "# Needed for many plotting methods; Setting the region names and their corresponding indices in the nodes list,\n",
    "# in this case it is just the label id - 1\n",
    "parcel_approach[\"Custom\"][\"regions\"] = {\n",
    "    \"Primary Visual\": {\"lh\": [0], \"rh\": [180]},\n",
    "    \"Early Visual\": {\"lh\": [1, 2, 3], \"rh\": [181, 182, 183]},\n",
    "    \"Dorsal Stream Visual\": {\"lh\": range(4, 10), \"rh\": range(184, 190)},\n",
    "    \"Ventral Stream Visual\": {\"lh\": range(10, 17), \"rh\": range(190, 197)},\n",
    "    \"MT+ Complex\": {\"lh\": range(17, 26), \"rh\": range(197, 206)},\n",
    "    \"SomaSens Motor\": {\"lh\": range(26, 31), \"rh\": range(206, 211)},\n",
    "    \"ParaCentral MidCing\": {\"lh\": range(31, 40), \"rh\": range(211, 220)},\n",
    "    \"Premotor\": {\"lh\": range(40, 47), \"rh\": range(220, 227)},\n",
    "    \"Posterior Opercular\": {\"lh\": range(47, 52), \"rh\": range(227, 232)},\n",
    "    \"Early Auditory\": {\"lh\": range(52, 59), \"rh\": range(232, 239)},\n",
    "    \"Auditory Association\": {\"lh\": range(59, 67), \"rh\": range(239, 247)},\n",
    "    \"Insula FrontalOperc\": {\"lh\": range(67, 79), \"rh\": range(247, 259)},\n",
    "    \"Medial Temporal\": {\"lh\": range(79, 87), \"rh\": range(259, 267)},\n",
    "    \"Lateral Temporal\": {\"lh\": range(87, 95), \"rh\": range(267, 275)},\n",
    "    \"TPO\": {\"lh\": range(95, 100), \"rh\": range(275, 280)},\n",
    "    \"Superior Parietal\": {\"lh\": range(100, 110), \"rh\": range(280, 290)},\n",
    "    \"Inferior Parietal\": {\"lh\": range(110, 120), \"rh\": range(290, 300)},\n",
    "    \"Posterior Cingulate\": {\"lh\": range(120, 133), \"rh\": range(300, 313)},\n",
    "    \"AntCing MedPFC\": {\"lh\": range(133, 149), \"rh\": range(313, 329)},\n",
    "    \"OrbPolaFrontal\": {\"lh\": range(149, 158), \"rh\": range(329, 338)},\n",
    "    \"Inferior Frontal\": {\"lh\": range(158, 167), \"rh\": range(338, 347)},\n",
    "    \"Dorsolateral Prefrontal\": {\"lh\": range(167, 180), \"rh\": range(347, 360)},\n",
    "    \"Subcortical Regions\": {\"lh\": range(360, 393), \"rh\": range(393, 426)},\n",
    "}\n",
    "\n",
    "# Saving the dictionary as a pickle file for long-term storage\n",
    "with open(os.path.join(demo_dir, \"HCPex_dict.pkl\"), \"wb\") as f:\n",
    "    joblib.dump(parcel_approach, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing BOLD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting updated parcellation approach\n",
    "extractor.parcel_approach = parcel_approach\n",
    "\n",
    "extractor.visualize_bold(subj_id=\"0004\", run=1, region=\"TPO\", figsize=(5, 4), output_dir=demo_dir, filename=\"HCPex_TPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting CAPs\n",
    "\n",
    "The following code uses ``CAP.get_bold`` to extract two CAPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurocaps.analysis import CAP\n",
    "\n",
    "# Will use the setter method to set the parcel approach later since a parcel approach\n",
    "# does not need to be defined for the `get_caps` method\n",
    "cap_analysis = CAP(parcel_approach=None)\n",
    "\n",
    "# Either method works\n",
    "cap_analysis.get_caps(subject_timeseries=extractor.subject_timeseries, n_clusters=2)\n",
    "\n",
    "# Alternative approach using pickle file:\n",
    "# cap_analysis.get_caps(subject_timeseries=\"openneuro_data.pkl\", n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Plotting with and without KNN Interpolation\n",
    "\n",
    "For the following code, the CAPs will be plotted to surface space with and without KNN (K-Nearest Neighbors)\n",
    "interpolation. Some parcellations may have issues projecting from MNI space to fsLR space. The ``knn_dict`` parameter,\n",
    "which is available in both ``CAP.caps2niftis()`` and ``CAP.caps2surf()`` can be used to improve the visualization. The\n",
    "KNN method uses a reference atlas (either Schaefer or AAL) as a mask to determine the non-background voxels to\n",
    "interpolate prior to projecting from MNI to fsLR space. *Note, for this method, only the \"maps\" subkey is required, the\n",
    "other subkeys are optional*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parcellation approach using pickle file\n",
    "cap_analysis.parcel_approach = os.path.join(demo_dir, \"HCPex_dict.pkl\")\n",
    "\n",
    "# Without KNN interpolation\n",
    "cap_analysis.caps2surf(\n",
    "    size=(500, 100),\n",
    "    layout=\"row\",\n",
    "    color_range=[-1, 1],\n",
    "    output_dir=demo_dir,\n",
    "    suffix_title=\"- No KNN Interpolation\",\n",
    "    suffix_filename=\"original\",\n",
    ")\n",
    "\n",
    "# With KNN interpolation\n",
    "cap_analysis.caps2surf(\n",
    "    size=(500, 100),\n",
    "    layout=\"row\",\n",
    "    color_range=[-1, 1],\n",
    "    knn_dict={\"k\": 5, \"reference_atlas\": \"Schaefer\"},\n",
    "    output_dir=demo_dir,\n",
    "    suffix_title=\"- With KNN Interpolation\",\n",
    "    suffix_filename=\"KNN\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
