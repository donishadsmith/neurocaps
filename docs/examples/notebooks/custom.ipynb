{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Using Custom Parcellations\n",
    "While NeuroCAPs leverages Nilearn's fetch functions for the [Schaefer](https://nilearn.github.io/stable/modules/generated/nilearn.datasets.fetch_atlas_schaefer_2018.html)\n",
    "and [AAL](https://nilearn.github.io/stable/modules/generated/nilearn.datasets.fetch_atlas_aal.html), additional\n",
    "parcellations (lateralized and non-lateralized) can be manually defined. For custom parcellation approaches, three subkeys are\n",
    "recognized: \"maps\", \"nodes\", and \"regions\". For additional details on these subkeys, refer to the\n",
    "[\"Custom Parcellations\" sub-section](https://neurocaps.readthedocs.io/en/stable/user_guide/parcellations.html#custom-parcellations).\n",
    "\n",
    "For this demonstration, the extended Human Connectome Project multimodal parcellation (HCPex) from\n",
    "[wayalan's Github](https://github.com/wayalan/HCPex/) will be used.\n",
    "\n",
    "**Note:** Non-lateralized parcellations are supported in versions >= 0.30.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "try:\n",
    "    import neurocaps\n",
    "except:\n",
    "    !pip install neurocaps[windows,demo]\n",
    "\n",
    "# Set headless display for google colab\n",
    "import os, sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"DISPLAY\"] = \":0.0\"\n",
    "    !apt-get install -y xvfb\n",
    "    !Xvfb :0 -screen 0 1024x768x24 &> /dev/null &\n",
    "    !Xvfb :0 -screen 0 1024x768x24 &> /dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Custom Parcellation Approach\n",
    "\n",
    "**Note**: Only \"maps\" subkey is needed for timeseries extraction\n",
    "\n",
    "For visualization methods in the ``TimeseriesExtractor`` and ``CAP`` classes, the nodes and regions need to be defined.\n",
    "Refer to the documentation for each function to determine which subkeys are required, as some methods only need the\n",
    "\"maps\" subkey, while others require the \"nodes\" and \"regions\" subkeys.\n",
    "\n",
    "The following code defines a custom parcellation approach using the HCPex parcellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching atlas NiFTI image and labels from Github\n",
    "import os, subprocess, sys\n",
    "\n",
    "demo_dir = \"neurocaps_demo\"\n",
    "os.makedirs(demo_dir, exist_ok=True)\n",
    "\n",
    "if sys.platform != \"win32\":\n",
    "    cmd = [\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"-P\",\n",
    "            demo_dir,\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex_LookUpTable.txt\",\n",
    "        ],\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"-P\",\n",
    "            demo_dir,\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex.nii.gz\",\n",
    "        ],\n",
    "    ]\n",
    "else:\n",
    "    cmd = [\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\",\n",
    "            f\"{demo_dir}\\\\HCPex_LookUpTable.txt\",\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex_LookUpTable.txt\",\n",
    "        ],\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\",\n",
    "            f\"{demo_dir}\\\\HCPex.nii.gz\",\n",
    "            \"https://github.com/wayalan/HCPex/raw/main/HCPex_v1.1/HCPex.nii.gz\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "for command in cmd:\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from neurocaps.utils import generate_custom_parcel_approach\n",
    "\n",
    "parcel_approach = generate_custom_parcel_approach(\n",
    "    os.path.join(demo_dir, \"HCPex_LookUpTable.txt\"),\n",
    "    maps_path=os.path.join(demo_dir, \"HCPex.nii.gz\"),\n",
    "    column_map={\"nodes\": \"Label\"},\n",
    "    background_label = \"Unknown\",\n",
    ")\n",
    "\n",
    "# The metadata for the labels are not mapped onto a region so this is done manually based\n",
    "# on published manuscript associated with this parcellation\n",
    "# Setting the region names and their corresponding indices in the nodes list\n",
    "# in this case it is just the label id - 1\n",
    "parcel_approach[\"Custom\"][\"regions\"] = {\n",
    "    \"Primary Visual\": {\"lh\": [0], \"rh\": [180]},\n",
    "    \"Early Visual\": {\"lh\": [1, 2, 3], \"rh\": [181, 182, 183]},\n",
    "    \"Dorsal Stream Visual\": {\"lh\": range(4, 10), \"rh\": range(184, 190)},\n",
    "    \"Ventral Stream Visual\": {\"lh\": range(10, 17), \"rh\": range(190, 197)},\n",
    "    \"MT+ Complex\": {\"lh\": range(17, 26), \"rh\": range(197, 206)},\n",
    "    \"SomaSens Motor\": {\"lh\": range(26, 31), \"rh\": range(206, 211)},\n",
    "    \"ParaCentral MidCing\": {\"lh\": range(31, 40), \"rh\": range(211, 220)},\n",
    "    \"Premotor\": {\"lh\": range(40, 47), \"rh\": range(220, 227)},\n",
    "    \"Posterior Opercular\": {\"lh\": range(47, 52), \"rh\": range(227, 232)},\n",
    "    \"Early Auditory\": {\"lh\": range(52, 59), \"rh\": range(232, 239)},\n",
    "    \"Auditory Association\": {\"lh\": range(59, 67), \"rh\": range(239, 247)},\n",
    "    \"Insula FrontalOperc\": {\"lh\": range(67, 79), \"rh\": range(247, 259)},\n",
    "    \"Medial Temporal\": {\"lh\": range(79, 87), \"rh\": range(259, 267)},\n",
    "    \"Lateral Temporal\": {\"lh\": range(87, 95), \"rh\": range(267, 275)},\n",
    "    \"TPO\": {\"lh\": range(95, 100), \"rh\": range(275, 280)},\n",
    "    \"Superior Parietal\": {\"lh\": range(100, 110), \"rh\": range(280, 290)},\n",
    "    \"Inferior Parietal\": {\"lh\": range(110, 120), \"rh\": range(290, 300)},\n",
    "    \"Posterior Cingulate\": {\"lh\": range(120, 133), \"rh\": range(300, 313)},\n",
    "    \"AntCing MedPFC\": {\"lh\": range(133, 149), \"rh\": range(313, 329)},\n",
    "    \"OrbPolaFrontal\": {\"lh\": range(149, 158), \"rh\": range(329, 338)},\n",
    "    \"Inferior Frontal\": {\"lh\": range(158, 167), \"rh\": range(338, 347)},\n",
    "    \"Dorsolateral Prefrontal\": {\"lh\": range(167, 180), \"rh\": range(347, 360)},\n",
    "    \"Subcortical Regions\": {\"lh\": range(360, 393), \"rh\": range(393, 426)},\n",
    "}\n",
    "\n",
    "# Saving the dictionary as a pickle file for long-term storage\n",
    "with open(os.path.join(demo_dir, \"HCPex_dict.pkl\"), \"wb\") as f:\n",
    "    joblib.dump(parcel_approach, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below fetches a single subject from an [OpenNeuro dataset](https://openneuro.org/datasets/ds005381/versions/1.0.0)\n",
    "preprocessed with [fMRIPrep](https://fmriprep.org/en/stable/). Downloading data from OpenNeuro requires\n",
    "``pip install openneuro-py ipywidgets`` or ``pip install neurocaps[demo]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Dataset] doi: doi:10.18112/openneuro.ds005381.v1.0.0\n",
    "from openneuro import download\n",
    "\n",
    "# Include the run-1 and run-2 data of a single subject\n",
    "include = [\n",
    "    \"dataset_description.json\",\n",
    "    \"sub-0004/ses-2/func/*run-[12]*events*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]*confounds_timeseries*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]_space-MNI152NLin*preproc_bold*\",\n",
    "]\n",
    "\n",
    "download(\n",
    "    dataset=\"ds005381\",\n",
    "    include=include,\n",
    "    target_dir=demo_dir,\n",
    "    verify_hash=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first level of the pipeline directory must also have a dataset_description.json file for querying purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "desc = {\n",
    "    \"Name\": \"fMRIPrep - fMRI PREProcessing workflow\",\n",
    "    \"BIDSVersion\": \"1.0.0\",\n",
    "    \"DatasetType\": \"derivative\",\n",
    "    \"GeneratedBy\": [\n",
    "        {\"Name\": \"fMRIPrep\", \"Version\": \"20.2.0\", \"CodeURL\": \"https://github.com/nipreps/fmriprep\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(\n",
    "    \"neurocaps_demo/derivatives/fmriprep/dataset_description.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump(desc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neurocaps.extraction import TimeseriesExtractor\n",
    "\n",
    "extractor = TimeseriesExtractor(\n",
    "    space=\"MNI152NLin6Asym\",\n",
    "    parcel_approach=parcel_approach,\n",
    "    standardize=True,\n",
    "    use_confounds=True,\n",
    "    confound_names=\"basic\",\n",
    "    low_pass=0.15,\n",
    "    high_pass=None,\n",
    "    dummy_scans=\"auto\",\n",
    "    fd_threshold={\n",
    "        \"threshold\": 0.5,\n",
    "        \"outlier_percentage\": 0.30,\n",
    "        \"use_sample_mask\": True,\n",
    "        \"interpolate\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Using chaining to extract timeseries data and save dictionary as a pickle file\n",
    "extractor.get_bold(\n",
    "    bids_dir=\"neurocaps_demo\",\n",
    "    session=\"2\",\n",
    "    task=\"DET\",\n",
    "    condition=\"late\",\n",
    "    condition_tr_shift=2,\n",
    "    slice_time_ref=0.5,\n",
    "    n_cores=None,\n",
    "    flush=True,\n",
    "    tr=2,\n",
    "    verbose=True,\n",
    ").timeseries_to_pickle(output_dir=demo_dir, filename=\"openneuro_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing BOLD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting updated parcellation approach\n",
    "extractor.parcel_approach = parcel_approach\n",
    "\n",
    "extractor.visualize_bold(\n",
    "    subj_id=\"0004\", run=1, region=\"TPO\", figsize=(5, 4), output_dir=demo_dir, filename=\"HCPex_TPO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting CAPs\n",
    "\n",
    "The following code uses ``CAP.get_bold`` to extract two CAPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurocaps.analysis import CAP\n",
    "\n",
    "# Will use the setter method to set the parcel approach later since a parcel approach\n",
    "# does not need to be defined for the `get_caps` method\n",
    "cap_analysis = CAP(parcel_approach=None)\n",
    "\n",
    "# Either method works\n",
    "cap_analysis.get_caps(subject_timeseries=extractor.subject_timeseries, n_clusters=2)\n",
    "\n",
    "# Alternative approach using pickle file:\n",
    "# cap_analysis.get_caps(subject_timeseries=\"openneuro_data.pkl\", n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Plotting with and without KNN Interpolation\n",
    "\n",
    "For the following code, the CAPs will be plotted to surface space with and without KNN (K-Nearest Neighbors)\n",
    "interpolation. Some parcellations may have issues projecting from MNI space to fsLR space. The ``knn_dict`` parameter,\n",
    "which is available in both ``CAP.caps2niftis()`` and ``CAP.caps2surf()`` can be used to improve the visualization. The\n",
    "KNN method uses a reference atlas (either Schaefer or AAL) as a mask to determine the non-background voxels to\n",
    "interpolate prior to projecting from MNI to fsLR space. *Note, for this method, only the \"maps\" subkey is required, the\n",
    "other subkeys are optional*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parcellation approach using pickle file\n",
    "cap_analysis.parcel_approach = os.path.join(demo_dir, \"HCPex_dict.pkl\")\n",
    "\n",
    "# Without KNN interpolation\n",
    "cap_analysis.caps2surf(\n",
    "    size=(500, 100),\n",
    "    layout=\"row\",\n",
    "    color_range=[-1, 1],\n",
    "    output_dir=demo_dir,\n",
    "    suffix_title=\"- No KNN Interpolation\",\n",
    "    suffix_filename=\"original\",\n",
    ")\n",
    "\n",
    "# With KNN interpolation\n",
    "cap_analysis.caps2surf(\n",
    "    size=(500, 100),\n",
    "    layout=\"row\",\n",
    "    color_range=[-1, 1],\n",
    "    knn_dict={\"k\": 5, \"reference_atlas\": \"Schaefer\"},\n",
    "    output_dir=demo_dir,\n",
    "    suffix_title=\"- With KNN Interpolation\",\n",
    "    suffix_filename=\"KNN\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Example with Schaefer 4S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, sys, subprocess\n",
    "from neurocaps.utils import generate_custom_parcel_approach\n",
    "\n",
    "# Fetching atlas NiFTI image and labels from Github\n",
    "if sys.platform != \"win32\":\n",
    "    cmd = [\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"-P\",\n",
    "            \"neurocaps_demo\",\n",
    "            \"https://github.com/PennLINC/AtlasPack/raw/main/atlas-4S156Parcels_dseg.tsv\",\n",
    "        ],\n",
    "    ]\n",
    "else:\n",
    "    cmd = [\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\",\n",
    "            \"neurocaps_demo\\\\atlas-4S156Parcels_dseg.tsv\",\n",
    "            \"https://github.com/PennLINC/AtlasPack/raw/main/atlas-4S156Parcels_dseg.tsv\",\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "for command in cmd:\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# For this parcellation, the metadata contains the labels and the network mappings though\n",
    "# certain nodes in the Cerebellum, Subcortical, and Thalamus have NaN values in the\n",
    "# column denoting network affiliation\n",
    "df = pd.read_csv(\n",
    "        r\"neurocaps_demo\\atlas-4S156Parcels_dseg.tsv\",\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "# Replacing null values in the \"network_label\" column with values in \"atlas_name\"\n",
    "df[\"network_label\"] = np.where(\n",
    "    df[\"network_label\"].isnull(), \n",
    "    df[\"atlas_name\"],            \n",
    "    df[\"network_label\"] \n",
    ")\n",
    "\n",
    "# Simplifying names for for certain names in \"network_label\"\n",
    "df.loc[df[\"network_label\"].str.contains(\n",
    "    \"Subcortical\", na=False), \"network_label\"] = \"Subcortical\"\n",
    "df.loc[df[\"network_label\"].str.contains(\"Thalamus\", na=False), \"network_label\"] = \"Thalamus\"\n",
    "\n",
    "# Create empty file for demonstration purposes\n",
    "with open(r\"neurocaps_demo\\temp_parc_map.nii.gz\", \"w\") as f:\n",
    "    pass\n",
    "\n",
    "# Creating custom parcel approach dictionary\n",
    "parcel_approach = generate_custom_parcel_approach(\n",
    "    df,\n",
    "    maps_path=r\"neurocaps_demo\\temp_parc_map.nii.gz\",\n",
    "    column_map={\"nodes\": \"label\", \"regions\": \"network_label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a lateralized version of the ``parcel_approach``. Note that the\n",
    "lateralization information is specific case in ``CAP.caps2plot`` when ``visual_scope`` is set to\n",
    "\"nodes\" and the ``add_custom_node_labels`` kwarg is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hemisphere column\n",
    "df[\"hemisphere_labels\"] = df[\"hemisphere_labels\"] = df[\"label\"].str.extract(r\"^(LH|RH)\")\n",
    "\n",
    "# Creating custom parcel approach dictionary\n",
    "parcel_approach = generate_custom_parcel_approach(\n",
    "    df,\n",
    "    maps_path=r\"neurocaps_demo\\temp_parc_map.nii.gz\",\n",
    "    column_map={\"nodes\": \"label\", \"regions\": \"network_label\", \"hemispheres\": \"hemisphere_labels\"},\n",
    "    hemisphere_map={\"lh\": [\"LH\"], \"rh\": [\"RH\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sub_ids = [f\"0{x}\" if x < 10 else x for x in range(1, 11)]\n",
    "subject_timeseries = {\n",
    "    str(x): {f\"run-{y}\": np.random.rand(50, 156) for y in range(1, 4)} for x in sub_ids\n",
    "}\n",
    "\n",
    "cap_analysis = CAP(parcel_approach=None)\n",
    "cap_analysis.parcel_approach = parcel_approach\n",
    "cap_analysis.get_caps(subject_timeseries=subject_timeseries, n_clusters=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.diverging_palette(145, 300, s=60, as_cmap=True)\n",
    "\n",
    "palette = sns.diverging_palette(260, 10, s=80, l=55, n=256, as_cmap=True)\n",
    "\n",
    "cap_analysis.caps2plot(visual_scope=\"regions\", plot_options=\"heatmap\", borderwidths=10)\n",
    "\n",
    "cap_analysis.caps2plot(\n",
    "    visual_scope=\"regions\",\n",
    "    plot_options=\"outer_product\",\n",
    "    subplots=True,\n",
    "    fontsize=14,\n",
    "    tight_layout=False,\n",
    "    xlabel_rotation=90,\n",
    "    hspace=0.3,\n",
    "    cmap=palette,\n",
    "    output_dir=\"neurocaps_demo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radialaxis = {\n",
    "    \"showline\": True,\n",
    "    \"linewidth\": 2,\n",
    "    \"linecolor\": \"rgba(0, 0, 0, 0.25)\",\n",
    "    \"gridcolor\": \"rgba(0, 0, 0, 0.25)\",\n",
    "    \"ticks\": \"outside\",\n",
    "    \"tickfont\": {\"size\": 14, \"color\": \"black\"},\n",
    "    \"range\": [0, 0.5],\n",
    "    \"tickvals\": [0.1, \"\", 0.3, \"\", 0.5],\n",
    "}\n",
    "\n",
    "color_discrete_map = {\n",
    "    \"High Amplitude\": \"rgba(255, 165, 0, 0.75)\",\n",
    "    \"Low Amplitude\": \"black\",\n",
    "}\n",
    "\n",
    "cap_analysis.caps2radar(\n",
    "    radialaxis=radialaxis,\n",
    "    fill=\"toself\",\n",
    "    color_discrete_map=color_discrete_map,\n",
    "    use_scatterpolar=True,\n",
    "    output_dir=\"neurocaps_demo\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
