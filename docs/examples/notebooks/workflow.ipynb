{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58f940e",
   "metadata": {},
   "source": [
    "# Tutorial 8: Workflow Example\n",
    "\n",
    "This tutorial demonstrates an example workflow from timeseries extraction to CAPs visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62138ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "try:\n",
    "    import neurocaps\n",
    "except:\n",
    "    !pip install neurocaps[windows,demo]\n",
    "\n",
    "# Set headless display for google colab\n",
    "import os, sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"DISPLAY\"] = \":0.0\"\n",
    "    !apt-get install -y xvfb\n",
    "    !Xvfb :0 -screen 0 1024x768x24 &> /dev/null &\n",
    "    !Xvfb :0 -screen 0 1024x768x24 &> /dev/null &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "demo_dir = \"neurocaps_demo\"\n",
    "os.makedirs(demo_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c03319",
   "metadata": {},
   "source": [
    "The code below fetches two subjects from an [OpenNeuro dataset](https://openneuro.org/datasets/ds005381/versions/1.0.0)\n",
    "preprocessed with [fMRIPrep](https://fmriprep.org/en/stable/). Downloading data from OpenNeuro requires\n",
    "``pip install openneuro-py ipywidgets`` or ``pip install neurocaps[demo]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Dataset] doi: doi:10.18112/openneuro.ds005381.v1.0.0\n",
    "from openneuro import download\n",
    "\n",
    "# Include the run-1 and run-2 data from two subjects\n",
    "include = [\n",
    "    \"dataset_description.json\",\n",
    "    \"sub-0004/ses-2/func/*run-[12]*events*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]*confounds_timeseries*\",\n",
    "    \"derivatives/fmriprep/sub-0004/fmriprep/sub-0004/ses-2/func/*run-[12]_space-MNI152NLin*preproc_bold*\",\n",
    "    \"sub-0006/ses-2/func/*run-[12]*events*\",\n",
    "    \"derivatives/fmriprep/sub-0006/fmriprep/sub-0006/ses-2/func/*run-[12]*confounds_timeseries*\",\n",
    "    \"derivatives/fmriprep/sub-0006/fmriprep/sub-0006/ses-2/func/*run-[12]_space-MNI152NLin*preproc_bold*\",\n",
    "]\n",
    "\n",
    "download(\n",
    "    dataset=\"ds005381\",\n",
    "    include=include,\n",
    "    target_dir=demo_dir,\n",
    "    verify_hash=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbeb5f",
   "metadata": {},
   "source": [
    "The first level of the pipeline directory must also have a dataset_description.json file for querying purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b935f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "desc = {\n",
    "    \"Name\": \"fMRIPrep - fMRI PREProcessing workflow\",\n",
    "    \"BIDSVersion\": \"1.0.0\",\n",
    "    \"DatasetType\": \"derivative\",\n",
    "    \"GeneratedBy\": [\n",
    "        {\"Name\": \"fMRIPrep\", \"Version\": \"20.2.0\", \"CodeURL\": \"https://github.com/nipreps/fmriprep\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(\n",
    "    \"neurocaps_demo/derivatives/fmriprep/dataset_description.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump(desc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurocaps.extraction import TimeseriesExtractor\n",
    "from neurocaps.utils import fetch_preset_parcel_approach\n",
    "\n",
    "# List of fMRIPrep-derived confounds for nuisance regression\n",
    "confound_names = [\n",
    "    \"cosine*\",\n",
    "    \"trans_x\",\n",
    "    \"trans_x_derivative1\",\n",
    "    \"trans_y\",\n",
    "    \"trans_y_derivative1\",\n",
    "    \"trans_z\",\n",
    "    \"trans_z_derivative1\",\n",
    "    \"rot_x\",\n",
    "    \"rot_x_derivative1\",\n",
    "    \"rot_y\",\n",
    "    \"rot_y_derivative1\",\n",
    "    \"rot_z\",\n",
    "    \"rot_z_derivative1\",\n",
    "    \"a_comp_cor_00\",\n",
    "    \"a_comp_cor_01\",\n",
    "    \"a_comp_cor_02\",\n",
    "    \"a_comp_cor_03\",\n",
    "    \"a_comp_cor_04\",\n",
    "    \"global_signal\",\n",
    "    \"global_signal_derivative1\",\n",
    "]\n",
    "\n",
    "# Initialize extractor with signal cleaning parameters\n",
    "extractor = TimeseriesExtractor(\n",
    "    space=\"MNI152NLin6Asym\",\n",
    "    parcel_approach=fetch_preset_parcel_approach(\"4S\", n_nodes=456),\n",
    "    standardize=True,\n",
    "    confound_names=confound_names,\n",
    "    fd_threshold={\n",
    "        \"threshold\": 0.50,\n",
    "        \"outlier_percentage\": 0.30,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Extract BOLD data from preprocessed fMRIPrep data\n",
    "# which should be located in the \"derivatives\" folder\n",
    "# within the BIDS root directory\n",
    "# The extracted timeseries data is automatically stored\n",
    "# Session 2 is the only session available, so `session`\n",
    "# does not need to be specified\n",
    "extractor.get_bold(\n",
    "    bids_dir=demo_dir,\n",
    "    task=\"DET\",\n",
    "    condition=\"late\",\n",
    "    condition_tr_shift=4,\n",
    "    tr=2,\n",
    "    verbose=False,\n",
    ").timeseries_to_pickle(demo_dir, \"timeseries.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dataframe containing QC information for each subject\n",
    "# to use for downstream statistical analyses\n",
    "qc_df = extractor.report_qc()\n",
    "print(qc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BOLD Data\n",
    "extractor.visualize_bold(subj_id=\"0004\", run=1, region=\"Vis\", figsize=(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurocaps.analysis import CAP\n",
    "\n",
    "# Initialize CAP class\n",
    "cap_analysis = CAP(parcel_approach=extractor.parcel_approach)\n",
    "\n",
    "# Identify the optimal number of CAPs (clusters)\n",
    "# using the variance_ratio method to test 2-20\n",
    "# The optimal number of CAPs is automatically stored\n",
    "cap_analysis.get_caps(\n",
    "    subject_timeseries=extractor.subject_timeseries,\n",
    "    n_clusters=range(2, 21),\n",
    "    standardize=True,\n",
    "    cluster_selection_method=\"variance_ratio\",\n",
    "    max_iter=500,\n",
    "    n_init=10,\n",
    "    random_state=0,\n",
    "    show_figs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53afe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal fraction and transition probability of each CAP for all subjects\n",
    "output = cap_analysis.calculate_metrics(\n",
    "    extractor.subject_timeseries, metrics=[\"temporal_fraction\", \"transition_probability\"]\n",
    ")\n",
    "print(output[\"temporal_fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d246b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged transition probability matrix\n",
    "from neurocaps.analysis import transition_matrix\n",
    "transition_matrix(output[\"transition_probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_analysis.caps2plot(plot_options=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project CAPs onto surface plots\n",
    "# and generate cosine similarity network alignment of CAPs\n",
    "\n",
    "radialaxis = {\n",
    "    \"showline\": True,\n",
    "    \"linewidth\": 2,\n",
    "    \"linecolor\": \"rgba(0, 0, 0, 0.25)\",\n",
    "    \"gridcolor\": \"rgba(0, 0, 0, 0.25)\",\n",
    "    \"ticks\": \"outside\",\n",
    "    \"tickfont\": {\"size\": 14, \"color\": \"black\"},\n",
    "    \"range\": [0, 0.9],\n",
    "    \"tickvals\": [0.1, \"\", 0.3, \"\", 0.5, \"\", 0.7, \"\", 0.9],\n",
    "}\n",
    "\n",
    "color_discrete_map = {\n",
    "    \"High Amplitude\": \"rgba(255, 165, 0, 0.75)\",\n",
    "    \"Low Amplitude\": \"black\",\n",
    "}\n",
    "\n",
    "cap_analysis.caps2surf().caps2radar(radialaxis=radialaxis, color_discrete_map=color_discrete_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
