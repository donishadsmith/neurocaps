<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>neurocaps &mdash; neurocaps 0.9.9.post3 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=a8be93fa" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=f8c5243f"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            neurocaps
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">neurocaps</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><strong>neurocaps</strong></li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/introduction.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="neurocaps">
<h1><strong>neurocaps</strong><a class="headerlink" href="#neurocaps" title="Link to this heading"></a></h1>
<a class="reference external image-reference" href="https://doi.org/10.5281/zenodo.11645498"><img alt="DOI" src="https://img.shields.io/badge/DOI-10.5281%2Fzenodo.11642615-blue" /></a>
<p>This is a Python package designed to perform Co-activation Patterns (CAPs) analyses. It utilizes k-means clustering to group timepoints (TRs) into brain states, applicable to both resting-state and task-based fMRI data.
The package is compatible with data preprocessed using <strong>fMRIPrep</strong> and assumes your directory is BIDS-compliant, containing a derivatives folder with a pipeline folder (such as fMRIPrep) that holds the preprocessed BOLD data.</p>
</section>
<section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h1>
<p>This package contains two main classes: <code class="docutils literal notranslate"><span class="pre">TimeseriesExtractor</span></code> for extracting the timeseries, and <code class="docutils literal notranslate"><span class="pre">CAP</span></code> for performing the CAPs analysis.</p>
<p><strong>Note:</strong> When extracting the timeseries, this package uses either the Schaefer atlas, the Automated Anatomical Labeling (AAL) atlas, or a custom parcellation where all nodes have a left and right version (bilateral nodes).
The number of ROIs and networks for the Schaefer atlas can be adjusted with the parcel_approach parameter when initializing the <code class="docutils literal notranslate"><span class="pre">TimeseriesExtractor</span></code> class.</p>
<p>To modify it, you must use a nested dictionary, where the primary key is “Schaefer” and the sub-keys are “n_rois” and “yeo_networks”. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parcel_approach</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Schaefer&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_rois&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;yeo_networks&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;resolution_mm&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}}</span>
</pre></div>
</div>
<p>Similarly, the version of the AAL atlas can be modified using:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parcel_approach</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;AAL&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;SPM12&quot;</span><span class="p">}}</span>
</pre></div>
</div>
<p>If using a “Custom” parcellation approach, ensure each node in your dataset includes both left (lh) and right (rh) hemisphere versions (bilateral nodes).</p>
<section id="custom-key-structure">
<h2>Custom Key Structure:<a class="headerlink" href="#custom-key-structure" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">maps</span></code>: Directory path containing necessary parcellation files. Ensure files are in a supported format (e.g., .nii for NifTI files). For plotting purposes, this key is not required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nodes</span></code>:  List of all node labels used in your study, arranged in the exact order they correspond to indices in your parcellation files. Each label should match the parcellation index it represents. For example, if the parcellation label “0” corresponds to the left hemisphere visual cortex area 1, then “LH_Vis1” should occupy the 0th index in this list. This ensures that data extraction and analysis accurately reflect the anatomical regions intended. For timeseries extraction, this key is not required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regions</span></code>: Dictionary defining major brain regions. Each region should list node indices under “lh” and “rh” to specify left and right hemisphere nodes. For timeseries extraction, this key is not required.</p></li>
</ul>
</section>
<section id="example">
<h2>Example:<a class="headerlink" href="#example" title="Link to this heading"></a></h2>
<p>The provided example demonstrates setting up a custom parcellation containing nodes for the visual network (Vis) and hippocampus regions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parcel_approach</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Custom&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;maps&quot;</span><span class="p">:</span> <span class="s2">&quot;/location/to/parcellation.nii.gz&quot;</span><span class="p">,</span>
        <span class="s2">&quot;nodes&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;LH_Vis1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;LH_Vis2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;LH_Hippocampus&quot;</span><span class="p">,</span>
            <span class="s2">&quot;RH_Vis1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;RH_Vis2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;RH_Hippocampus&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;regions&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Vis&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;lh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="s2">&quot;rh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
            <span class="p">},</span>
            <span class="s2">&quot;Hippocampus&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;lh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="s2">&quot;rh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="main-features-for-timeseriesextractor-includes">
<h2>Main features for <code class="docutils literal notranslate"><span class="pre">TimeseriesExtractor</span></code> includes:<a class="headerlink" href="#main-features-for-timeseriesextractor-includes" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Timeseries Extraction:</strong> Extract timeseries for resting-state or task data, creating a nested dictionary containing the subject ID, run number, and associated timeseries. This serves as input for the <code class="docutils literal notranslate"><span class="pre">get_caps()</span></code> method in the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class.</p></li>
<li><p><strong>Saving Timeseries:</strong> Save the nested dictionary containing timeseries as a pickle file.</p></li>
<li><p><strong>Visualization:</strong> Visualize the timeseries of a Schaefer, AAL, or Custom parcellation node or region/network in a specific subject’s run, with options to save the plots.</p></li>
<li><p><strong>Parallel Processing:</strong> Use parallel processing by specifying the number of CPU cores in the <code class="docutils literal notranslate"><span class="pre">n_cores</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">get_bold()</span></code> method. Testing on an HPC using a loop with <code class="docutils literal notranslate"><span class="pre">TimeseriesExtractor.get_bold()</span></code> to extract session 1 and 2
BOLD timeseries from 105 subjects from resting-state data (single run containing 360 volumes) and two task datasets (three runs containing 200 volumes each and two runs containing 200 volumes) reduced processing time from 5 hours 48 minutes to 1 hour 26 minutes
(using 10 cores). <em>Note:</em> If you are using an HPC, remember to allocate the appropriate amount of CPU cores with your workload manager. For instance in slurm use <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--cpus-per-task=10</span></code> if you intend to use 10 cores.</p></li>
</ul>
</section>
<section id="main-features-for-cap-includes">
<h2>Main features for <code class="docutils literal notranslate"><span class="pre">CAP</span></code> includes:<a class="headerlink" href="#main-features-for-cap-includes" title="Link to this heading"></a></h2>
<ul>
<li><p><strong>Optimal Cluster Size Identification:</strong> Perform the silhouette or elbow method to identify the optimal cluster size, saving the optimal model as an attribute.</p></li>
<li><p><strong>Parallel Processing:</strong> Use parallel processing, when using the silhouette or elbow method, by specifying the number of CPU cores in the <code class="docutils literal notranslate"><span class="pre">n_cores</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">`get_caps()</span></code> method.
<em>Note:</em> If you are using an HPC, remember to allocate the appropriate amount of CPU cores with your workload manager. For instance in slurm use <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--cpus-per-task=10</span></code> if you intend to use 10 cores.</p></li>
<li><p><strong>Grouping:</strong> Perform CAPs analysis for entire sample or groups of subject IDs (using the <code class="docutils literal notranslate"><span class="pre">groups</span></code> parameter when initializing the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class). K-means clustering, silhouette and elbow methods, and plotting are done for each group when specified.</p></li>
<li><p><strong>CAP Visualization:</strong> Visualize the CAPs as outer products or heatmaps, with options to use subplots to reduce the number of individual plots, as well as save.
Refer to the <a class="reference external" href="https://neurocaps.readthedocs.io/en/latest/generated/neurocaps.analysis.CAP.html#neurocaps.analysis.CAP.caps2plot">documentation</a> for the <code class="docutils literal notranslate"><span class="pre">caps2plot()</span></code> method in the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class for available <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> arguments and parameters to modify plots.</p></li>
<li><p><strong>Save CAPs as NifTIs:</strong> Convert the atlas used for parcellation to a stat map and saves them (<code class="docutils literal notranslate"><span class="pre">caps2niftis</span></code>).</p></li>
<li><p><strong>Surface Plot Visualization:</strong> Convert the atlas used for parcellation to a stat map projected onto a surface plot with options to customize and save plots.
Refer to the <a class="reference external" href="https://neurocaps.readthedocs.io/en/latest/generated/neurocaps.analysis.CAP.html#neurocaps.analysis.CAP.caps2surf">documentation</a> for the <code class="docutils literal notranslate"><span class="pre">caps2surf()</span></code> method in the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class for available <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> arguments and parameters to modify plots.
Also includes the option to save the NifTIs. There is also another a parameter in <code class="docutils literal notranslate"><span class="pre">caps2surf</span></code>, <code class="docutils literal notranslate"><span class="pre">fslr_giftis_dict</span></code>, which can be used if the CAPs NifTI files were converted to GifTI files using a tool such as Connectome Workbench, which may work better for
converting your atlas to fslr space. This parameter allows plotting without re-running the analysis and only initializing the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class and using the <code class="docutils literal notranslate"><span class="pre">caps2surf</span></code> method is needed.</p></li>
<li><p><strong>Correlation Matrix Creation:</strong> Create a correlation matrix from CAPs with options to customize and save plots. Refer to the <a class="reference external" href="https://neurocaps.readthedocs.io/en/latest/generated/neurocaps.analysis.CAP.html#neurocaps.analysis.CAP.caps2corr">documentation</a>
for the <code class="docutils literal notranslate"><span class="pre">caps2corr()</span></code> method in the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class for available <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> arguments and parameters to modify plots.</p></li>
<li><dl>
<dt><strong>CAP Metrics Calculation:</strong> Calculate CAP metrics (<code class="docutils literal notranslate"><span class="pre">calculate_metrics()</span></code>) as described in <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2018.01.041">Liu et al., 2018</a> <a class="footnote-reference brackets" href="#id7" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2021.118193">Yang et al., 2021</a> <a class="footnote-reference brackets" href="#id8" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>:</dt><dd><ul>
<li><p><em>Temporal Fraction:</em> The proportion of total volumes spent in a single CAP over all volumes in a run.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_subject_timeseries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">temporal_fraction</span> <span class="o">=</span> <span class="mi">4</span><span class="o">/</span><span class="mi">6</span>
</pre></div>
</div>
</li>
<li><p><em>Persistence:</em> The average time spent in a single CAP before transitioning to another CAP (average consecutive/uninterrupted time).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_subject_timeseries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Sequences for 1 are [1] and [1,1,1]</span>
<span class="n">persistence</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># Average number of frames</span>
<span class="n">tr</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">if</span> <span class="n">tr</span><span class="p">:</span>
    <span class="n">persistence</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># Turns average frames into average time</span>
</pre></div>
</div>
</li>
<li><p><em>Counts:</em> The frequency of each CAP observed in a run.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_subject_timeseries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">counts</span> <span class="o">=</span> <span class="mi">4</span>
</pre></div>
</div>
</li>
<li><p><em>Transition Frequency:</em> The number of switches between different CAPs across the entire run.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_subject_timeseries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1"># Transitions between unique CAPs occur at indices 0 -&gt; 1, 1 -&gt; 2, and 4 -&gt; 5</span>
<span class="n">transition_frequency</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>Cosine Similarity Radar Plots:</strong> Create radar plots showing the cosine similarity between CAPs and networks/regions. Especially useful as a quantitative method to categorize CAPs by determining the regions containing the most nodes demonstrating
increased co-activation or decreased co-deactivation <a class="footnote-reference brackets" href="#id9" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. Refer to the <a class="reference external" href="https://neurocaps.readthedocs.io/en/latest/generated/neurocaps.analysis.CAP.html#neurocaps.analysis.CAP.caps2radar">documentation</a> in <code class="docutils literal notranslate"><span class="pre">caps2radar</span></code> in the <code class="docutils literal notranslate"><span class="pre">CAP</span></code> class for a more
detailed explanation as well as available <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> arguments and parameters to modify plots. <strong>Note</strong>, the “Low Amplitude”are negative cosine similarity values. The absolute value of those cosine similarities are taken so that the radar plot starts at 0 and magnitude
comparisons between the “High Amplitude” and “Low Amplitude” groups are easier to see. Below is an example of how the cosine similarity is calculated for this function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># Nodes in order of their label ID, &quot;LH_Vis1&quot; is the 0th index in the parcellation</span>
<span class="c1"># but has a label ID of 1, and RH_SomSot2 is in the 7th index but has a label ID</span>
<span class="c1"># of 8 in the parcellation.</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LH_Vis1&quot;</span><span class="p">,</span> <span class="s2">&quot;LH_Vis2&quot;</span><span class="p">,</span> <span class="s2">&quot;LH_SomSot1&quot;</span><span class="p">,</span> <span class="s2">&quot;LH_SomSot2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;RH_Vis1&quot;</span><span class="p">,</span> <span class="s2">&quot;RH_Vis2&quot;</span><span class="p">,</span> <span class="s2">&quot;RH_SomSot1&quot;</span><span class="p">,</span> <span class="s2">&quot;RH_SomSot2&quot;</span><span class="p">]</span>
<span class="c1"># Binary representation of the nodes in Vis, essentially acts as</span>
<span class="c1"># a mask isolating the modes for for Vis</span>
<span class="n">binary_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Cluster centroid for CAP 1</span>
<span class="n">cap_1_cluster_centroid</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span>
<span class="c1"># Dot product is the sum of all the values here [-0.3, 1.5, 0, 0, 0.7, 1.3, 0, 0]</span>
<span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">cap_1_cluster_centroid</span><span class="p">,</span> <span class="n">binary_vector</span><span class="p">)</span>

<span class="n">norm_cap_1_cluster_centroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">cap_1_cluster_centroid</span><span class="p">)</span>
<span class="n">norm_binary_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">binary_vector</span><span class="p">)</span>
<span class="c1"># Cosine similarity between CAP 1 and the visual network</span>
<span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">dot_product</span><span class="o">/</span><span class="p">(</span><span class="n">norm_cap_1_cluster_centroid</span> <span class="o">*</span> <span class="n">norm_binary_vector</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p><strong>Additionally, the `neurocaps.analysis` submodule contains two additional functions:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">merge_dicts</span></code>: Merge the subject_timeseries dictionaries for overlapping subjects across tasks to identify similar CAPs across different tasks. The merged dictionary can be saved as a pickle file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">standardize</span></code>: Standardizes each run independently for all subjects in the subject timeseries.</p></li>
</ul>
<p>Please refer to <a class="reference external" href="https://github.com/donishadsmith/neurocaps/blob/main/demo.ipynb">demo.ipynb</a> for a more extensive demonstration of the features included in this package.</p>
</section>
</section>
<section id="dependencies">
<h1>Dependencies<a class="headerlink" href="#dependencies" title="Link to this heading"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">neurocaps</span></code> relies on several packages:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dependencies</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;numpy&gt;=1.22.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pandas&gt;=2.0.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;joblib&gt;=1.3.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;matplotlib&gt;=3.6.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;seaborn&gt;=0.11.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;kneed&quot;</span><span class="p">,</span>
            <span class="s2">&quot;nibabel&gt;=3.2.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;nilearn&gt;=0.10.1, !=0.10.3&quot;</span><span class="p">,</span>
            <span class="s2">&quot;scikit-learn&gt;=1.4.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;surfplot&quot;</span><span class="p">,</span>
            <span class="s2">&quot;neuromaps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pybids&gt;=0.16.2; platform_system != &#39;Windows&#39;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;plotly&quot;</span><span class="p">,</span>
            <span class="s2">&quot;kaleido==0.1.0.post1; platform_system == &#39;Windows&#39;&quot;</span><span class="p">,</span> <span class="c1"># Plotly saving seems to work best with this version for Windows</span>
            <span class="s2">&quot;kaleido; platform_system != &#39;Windows&#39;&quot;</span>
            <span class="p">]</span>
</pre></div>
</div>
</section>
<section id="references">
<h1>References<a class="headerlink" href="#references" title="Link to this heading"></a></h1>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p>Liu, X., Zhang, N., Chang, C., &amp; Duyn, J. H. (2018). Co-activation patterns in resting-state fMRI signals. NeuroImage, 180, 485–494. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2018.01.041">https://doi.org/10.1016/j.neuroimage.2018.01.041</a></p>
</aside>
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Yang, H., Zhang, H., Di, X., Wang, S., Meng, C., Tian, L., &amp; Biswal, B. (2021). Reproducible coactivation patterns of functional brain networks reveal the aberrant dynamic state transition in schizophrenia. NeuroImage, 237, 118193. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2021.118193">https://doi.org/10.1016/j.neuroimage.2021.118193</a></p>
</aside>
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>Zhang, R., Yan, W., Manza, P., Shokri-Kojori, E., Demiral, S. B., Schwandt, M., Vines, L., Sotelo, D., Tomasi, D., Giddens, N. T., Wang, G., Diazgranados, N., Momenan, R., &amp; Volkow, N. D. (2023).
Disrupted brain state dynamics in opioid and alcohol use disorder: attenuation by nicotine use. Neuropsychopharmacology, 49(5), 876–884. <a class="reference external" href="https://doi.org/10.1038/s41386-023-01750-w">https://doi.org/10.1038/s41386-023-01750-w</a></p>
</aside>
</aside>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, neurocaps developers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>